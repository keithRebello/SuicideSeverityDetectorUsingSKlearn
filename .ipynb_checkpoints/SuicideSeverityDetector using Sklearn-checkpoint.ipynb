{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Suicide Severity Detector Using SKLearn\n",
    "\n",
    "This model uses the Reddit C-SSRS Suicide Dataset to identify whether a comment made is either Ideation, Supportive, Indicator, Behavior and Attempt.\n",
    "\n",
    "##### Citation for the Dataset:\n",
    "Gaur, Manas, Alambo, Amanuel, Sain, Joy Prakash, Kurscuncu, Ugur, Thirunarayan, Krishnaprasad, Kavuluru, Ramakanth, â€¦ Pathak, Jyotishman. (2019). Reddit C-SSRS Suicide Dataset. Zenodo. http://doi.org/10.5281/zenodo.2667859"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('500_Reddit_users_posts_labels.csv') #loading the dataset\n",
    "df.describe() #basic summary of the data\n",
    "labels = df.Label.unique() #storing the unique label types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fc94116a208>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.Label.value_counts().plot(kind='bar') #exploring the distribution of labels in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with the Class imbalance\n",
    "\n",
    "Since the dataset is clearly imbalanced, we'll need to balance the data to achieve a better result. \n",
    "One method to do this is to implment models using a balanced class weight. The weights of each class have been calculated and displayed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Ideation': 0.5847953216374269, 'Behavior': 1.2987012987012987, 'Indicator': 1.0101010101010102, 'Supportive': 0.9259259259259259, 'Attempt': 2.2222222222222223}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "class_weights = compute_class_weight('balanced', df.Label.unique(),df.Label)\n",
    "label_weights = dict(zip(df.Label.unique(),class_weights)) \n",
    "print(label_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Another method is to use resampling to balance the number of samples for each class by either upsampling the minority classes or downsampling the majority classes. First we take a look at the exactdistribution of classes and then we perform both upsampling and downsampling to compare results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ideation      171\n",
       "Supportive    108\n",
       "Indicator      99\n",
       "Behavior       77\n",
       "Attempt        45\n",
       "Name: Label, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils import resample\n",
    "df.Label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ideation      171\n",
       "Attempt       171\n",
       "Behavior      171\n",
       "Supportive    171\n",
       "Indicator     171\n",
       "Name: Label, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ideation = df[df.Label == 'Ideation']\n",
    "df_supportive = df[df.Label == 'Supportive']\n",
    "df_indicator = df[df.Label == 'Indicator']\n",
    "df_behavior = df[df.Label == 'Behavior']\n",
    "df_attempt = df[df.Label == \"Attempt\"]\n",
    "\n",
    "df_supportive_upsampled = resample(df_supportive, \n",
    "                                 replace=True,     # sample with replacement\n",
    "                                 n_samples=171,    # to match majority class\n",
    "                                 random_state=123)\n",
    "\n",
    "df_indicator_upsampled = resample(df_indicator, \n",
    "                                 replace=True,     # sample with replacement\n",
    "                                 n_samples=171,    # to match majority class\n",
    "                                 random_state=123) \n",
    "\n",
    "df_behavior_upsampled = resample(df_behavior, \n",
    "                                 replace=True,     # sample with replacement\n",
    "                                 n_samples=171,    # to match majority class\n",
    "                                 random_state=123) \n",
    "\n",
    "df_attempt_upsampled = resample(df_attempt, \n",
    "                                 replace=True,     # sample with replacement\n",
    "                                 n_samples=171,    # to match majority class\n",
    "                                 random_state=123)\n",
    "\n",
    "df_upsampled = pd.concat([df_ideation, df_attempt_upsampled,df_behavior_upsampled,df_indicator_upsampled,df_supportive_upsampled])\n",
    "df_upsampled.Label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additionally we're also peforming preprocessing to clean the data before we feed it to our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "stemmer= PorterStemmer()\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "posts = []\n",
    "for i in df_upsampled.Post:\n",
    "    lower = str(i).lower() # Convert text to lower case\n",
    "    numberless = re.sub(r\"\\d+\", \"\", lower) # remove all numbers\n",
    "    punctuations = numberless.translate(str.maketrans('','',string.punctuation)) #remove punctuations etc\n",
    "    whitespaces = punctuations.strip() #remove whitespaces\n",
    "    tokens = word_tokenize(whitespaces) #tokenize the post\n",
    "    result = [i for i in tokens if not i in stop_words] # remove stopwords\n",
    "    stem_word = [stemmer.stem(word) for word in result] # stemming the post\n",
    "    posts.append(\" \".join(stem_word))\n",
    "df_upsampled.Post = posts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now we spit the dataset and run different models to compare the results of each using a convenient method of sklearn metrics called a classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = df_upsampled.Post\n",
    "y = df_upsampled.Label\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model 1# Using a multinomial Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.642023346303502\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Supportive       0.46      1.00      0.63        51\n",
      "    Ideation       0.93      0.46      0.62        56\n",
      "    Behavior       0.47      0.35      0.40        51\n",
      "     Attempt       0.88      0.72      0.79        53\n",
      "   Indicator       0.89      0.70      0.78        46\n",
      "\n",
      "    accuracy                           0.64       257\n",
      "   macro avg       0.73      0.65      0.64       257\n",
      "weighted avg       0.73      0.64      0.64       257\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "nb = Pipeline([('vect', CountVectorizer()),\n",
    "               ('tfidf', TfidfTransformer()),\n",
    "               ('clf', MultinomialNB()),\n",
    "              ])\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "y_pred = nb.predict(X_test)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred,target_names=labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model 2# Using a multinomial Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.7782101167315175\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Supportive       1.00      0.88      0.94        51\n",
      "    Ideation       1.00      0.71      0.83        56\n",
      "    Behavior       0.52      0.75      0.61        51\n",
      "     Attempt       1.00      0.75      0.86        53\n",
      "   Indicator       0.63      0.80      0.70        46\n",
      "\n",
      "    accuracy                           0.78       257\n",
      "   macro avg       0.83      0.78      0.79       257\n",
      "weighted avg       0.84      0.78      0.79       257\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = Pipeline([('vect', CountVectorizer()),\n",
    "               ('tfidf', TfidfTransformer()),\n",
    "               ('clf', RandomForestClassifier(n_estimators=1600,max_depth=20)),\n",
    "              ])\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred,target_names=labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model 3# Using a multinomial Multi-Layer Perceptron Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.17898832684824903\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Supportive       0.00      0.00      0.00        51\n",
      "    Ideation       0.00      0.00      0.00        56\n",
      "    Behavior       0.00      0.00      0.00        51\n",
      "     Attempt       0.00      0.00      0.00        53\n",
      "   Indicator       0.18      1.00      0.30        46\n",
      "\n",
      "    accuracy                           0.18       257\n",
      "   macro avg       0.04      0.20      0.06       257\n",
      "weighted avg       0.03      0.18      0.05       257\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/keithrebello/.local/lib/python3.5/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "nb = Pipeline([('vect', CountVectorizer()),\n",
    "               ('tfidf', TfidfTransformer()),\n",
    "               ('clf', MLPClassifier(solver='lbfgs', alpha=1e-5,hidden_layer_sizes=(5, 2), random_state=1)),\n",
    "              ])\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "y_pred = nb.predict(X_test)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred,target_names=labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downsampling\n",
    "we now try our luck with downsampling by checking it with the best performing model in the upsampling section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ideation      45\n",
       "Behavior      45\n",
       "Supportive    45\n",
       "Indicator     45\n",
       "Attempt       45\n",
       "Name: Label, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_supportive_dsampled = resample(df_supportive, \n",
    "                                 replace=False,     # sample with replacement\n",
    "                                 n_samples=45,    # to match majority class\n",
    "                                 random_state=123)\n",
    "\n",
    "df_indicator_dsampled = resample(df_indicator, \n",
    "                                 replace=False,     # sample with replacement\n",
    "                                 n_samples=45,    # to match majority class\n",
    "                                 random_state=123) \n",
    "\n",
    "df_behavior_dsampled = resample(df_behavior, \n",
    "                                 replace=False,     # sample with replacement\n",
    "                                 n_samples=45,    # to match majority class\n",
    "                                 random_state=123) \n",
    "\n",
    "df_ideation_dsampled = resample(df_ideation, \n",
    "                                 replace=False,     # sample with replacement\n",
    "                                 n_samples=45,    # to match majority class\n",
    "                                 random_state=123)\n",
    "\n",
    "df_downsampled = pd.concat([df_ideation_dsampled, df_attempt,df_behavior_dsampled,df_indicator_dsampled,df_supportive_dsampled])\n",
    "df_downsampled.Label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Similar pre-precessing of the dataset as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer= PorterStemmer()\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "posts = []\n",
    "for i in df_downsampled.Post:\n",
    "    lower = str(i).lower()\n",
    "    numberless = re.sub(r\"\\d+\", \"\", lower)\n",
    "    punctuations = numberless.translate(str.maketrans('','',string.punctuation))\n",
    "    whitespaces = punctuations.strip()\n",
    "    tokens = word_tokenize(whitespaces)\n",
    "    result = [i for i in tokens if not i in stop_words]\n",
    "    stem_word = [stemmer.stem(word) for word in result]\n",
    "    '''for word in stem_word:\n",
    "        lemmatized = lemmatizer.lemmatize(word)'''\n",
    "    posts.append(\" \".join(stem_word))\n",
    "df_downsampled.Post = posts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing the model with downsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_downsampled.Post\n",
    "y = df_downsampled.Label\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.22058823529411764\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Supportive       0.28      0.38      0.32        13\n",
      "    Ideation       0.20      0.07      0.10        15\n",
      "    Behavior       0.12      0.17      0.14        12\n",
      "     Attempt       0.00      0.00      0.00        13\n",
      "   Indicator       0.32      0.47      0.38        15\n",
      "\n",
      "    accuracy                           0.22        68\n",
      "   macro avg       0.18      0.22      0.19        68\n",
      "weighted avg       0.19      0.22      0.19        68\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nb = Pipeline([('vect', CountVectorizer()),\n",
    "               ('tfidf', TfidfTransformer()),\n",
    "               ('clf', RandomForestClassifier()),\n",
    "              ])\n",
    "\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "y_pred = nb.predict(X_test)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred,target_names=labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The models can be further developed by tinkering with the hyperparameters of the model. \n",
    "I now try differentcombinations of parameters using ParameterGrid from sklearn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_upsampled.Post\n",
    "y = df_upsampled.Label\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "params = {\"n_estimators\":[400,800,1200,1600,2000],\"max_depth\":[2,4,5,6,10,20],\"oob_score\":[True,False]}\n",
    "param_list = list(ParameterGrid(params))\n",
    "\n",
    "acc=[]\n",
    "param_combo =[]\n",
    "for i in param_list:\n",
    "    rf = Pipeline([('vect', CountVectorizer()),\n",
    "               ('tfidf', TfidfTransformer()),\n",
    "               ('clf', RandomForestClassifier(**i)),\n",
    "              ])\n",
    "\n",
    "    rf.fit(X_train, y_train)\n",
    "    y_pred = rf.predict(X_test)\n",
    "    acc.append(accuracy_score(y_pred, y_test))\n",
    "    param_combo.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best accuracy 0.7821011673151751\n",
      "best combo is  {'n_estimators': 1200, 'oob_score': True, 'max_depth': 20}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Supportive       1.00      0.88      0.94        51\n",
      "    Ideation       1.00      0.71      0.83        56\n",
      "    Behavior       0.48      0.71      0.57        51\n",
      "     Attempt       1.00      0.74      0.85        53\n",
      "   Indicator       0.64      0.80      0.71        46\n",
      "\n",
      "    accuracy                           0.77       257\n",
      "   macro avg       0.82      0.77      0.78       257\n",
      "weighted avg       0.83      0.77      0.78       257\n",
      "\n"
     ]
    }
   ],
   "source": [
    "param_result = dict(zip(acc,param_combo))\n",
    "\n",
    "print('best accuracy %s' % max(acc))\n",
    "print('best combo is ',param_result[max(acc)])\n",
    "print(classification_report(y_test, y_pred,target_names=labels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
